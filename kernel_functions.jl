using MLJBase, MLJ, MLJModelInterface
using MLJLinearModels
using KernelFunctions, Distances, Statistics
using DataFrames

###############################################################################
#  Kernel functions utilized by MLJ.jl regressor functions 
# SingleKernelRegressor and MultipleKernelRegressor defined below.
#
###############################################################################

#base exponential p-norm kernel
struct PnormKernel <: KernelFunctions.SimpleKernel 
    pnorm::Real
end
KernelFunctions.kappa(::PnormKernel, d1::Real) = exp(-d1)
KernelFunctions.metric(self::PnormKernel) = Minkowski(self.pnorm)


# base p-norm kernel
# used to create distance matrix from which calculate optimal λ_kern
# based on "p" and standard deviation
struct PnormKernelStd <: KernelFunctions.SimpleKernel 
    pnorm::Real
end
KernelFunctions.kappa(::PnormKernelStd, d1::Real) = d1
KernelFunctions.metric(self::PnormKernelStd) = Minkowski(self.pnorm)


#p-norm function to generate kernel matrix, dispatch with one input vector
function PnormKernelFunction(data::Vector; p::Float64=1.0, λ_kern::Float64=1.0e-4)
"""
    PnormKernelFunction(x_vect, p, λ_kern)

Generate a kernel matrix (square matrix) that is generated by a the p-norm
or Minkowski function.  The p-norm is exponentially raised (changing from a
distance to similarity measurement). The core function is

``κ_{i,j}(x,y)=exp^{-λ\\|x-y\\|_p}``

where the (i,j)'th element of the kernel matrix is define. This is built 
on top of KernelFunctions.jl
"""
    #set up kernel trick function
    K = KernelFunctions.compose(PnormKernel(p), ScaleTransform(λ_kern))

    #employ kernel trick into vectored space
    #note that converted from table/dataframe to Vector/array and back to table
    return MLJ.table(kernelmatrix(K, Array(data)))
end 

#p-norm function to generate kernel matrix, dispatch with one input table/dataframe
function PnormKernelFunction(data::AbstractDataFrame; p::Float64=1.0, λ_kern::Float64=1.0e-4)
"""
    PnormKernelFunction(x_dataframe, p, λ_kern)

Generate a kernel matrix (square matrix) that is generated by a the p-norm
or Minkowski function.  The p-norm is exponentially raised (changing from a
distance to similarity measurement). The core function is

``κ_{i,j}(x,y)=exp^{-λ\\|x-y\\|_p}``

where the (i,j)'th element of the kernel matrix is define. This is built 
on top of KernelFunctions.jl
"""
    #set up kernel trick function
    K = KernelFunctions.compose(PnormKernel(p), ScaleTransform(λ_kern))

    #employ kernel trick into vectored space
    #note that converted from table/dataframe to matrix and back to table
    return MLJ.table(kernelmatrix(K, Matrix(data),obsdim=1))
end 

#p-norm function to generate kernel matrix, dispatch with two input tables/dataframes
function PnormKernelFunction(dataNew, dataOld; p::Float64=1.0, λ_kern::Float64=1.0e-4)
"""
    PnormKernelFunction(x_dataframe, y_dataframe, p, λ_kern)

Generate a kernel matrix (square matrix) that is generated by a the p-norm
or Minkowski function.  The p-norm is exponentially raised (changing from a
distance to similarity measurement). The core function is

``κ_{i,j}(x,y)=exp^{-λ\\|x-y\\|_p}``

where the (i,j)'th element of the kernel matrix is define. This is built 
on top of KernelFunctions.jl
"""
    #set up kernel trick function
    K = KernelFunctions.compose(PnormKernel(p), ScaleTransform(λ_kern))    

    #employ kernel trick into vectored space
    #note that converted from table/dataframe to matrix and back to table
    return MLJ.table(kernelmatrix(K, Matrix(dataNew), Matrix(dataOld), obsdim=1))
    
end


#kernel to realize Scikit-learn's version of Laplacian (L1) kernel
struct L1ExponentialKernel <: KernelFunctions.SimpleKernel end
KernelFunctions.kappa(::L1ExponentialKernel, d1::Real) = exp(-d1)
KernelFunctions.metric(::L1ExponentialKernel) = Cityblock()


###############################################################################
# Utility functions
#
###############################################################################

#helper function to convert MLJ paired symbol, float coef to simple
#float vector
function MLJcoef_to_vect(coefs::Vector{Pair{Symbol, Float64}})
"""
    MLJcoef_to_vect(opt.cache.coef)

Take a MLJ machine's cache coef vector which is a paired symbol
and float and convert it to simple float vector. Useful for plotting.
See MLJ.jl.
"""
    vect = Float64[]
    for idx in coefs
        vect = vcat(vect, idx[2])
    end
    return vect
end;

function maxk(a, k::Integer)
"""
    maxk(a_vector, k_int)

Mimic Matlab function to find largest 'k' values and returns 
associated index values 
"""
           b = partialsortperm(a, 1:k, rev=true)
           #return collect(zip(b, a[b]))
           return b
end
#a is vector to find min values, k is number of items
function mink(a, k::Integer)
"""
    mink(a_vector, k_int)

Mimic Matlab function to find smallest 'k' values and returns 
associated index values 
"""
           b = partialsortperm(a, 1:k, rev=false)
           #return collect(zip(b, a[b]))
           return b
end;

###############################################################################
# Single-kernel Regressor constructor based on MLJ.jl
#
# Once constructed, have standard 'fit' and 'predict' function calls
#
#https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/
#https://discourse.julialang.org/t/mlj-selecting-rows-and-columns-for-training-in-evaluate-for-kernel-regression/52832/8
#
#this is essentially a module if add in proper using/imports
###############################################################################

#const MMI = MLJBase #MLJModelInterface
const MMI = MLJModelInterface

#set up basic model structure, clean! and keyword construct using macro method
#basically a wrapper
@mlj_model mutable struct SingleKernelRegressor <: MMI.Deterministic
    p::Float64      = 1.0::(_ ≥ 0)     # p for p-norm distance used in pair-wise kernel matrix
    λ_kern::Float64 = 999.0::(_ ≥ 0)    # kernel scale parameter for X applied prior to kernelization
    mdl::Deterministic=RidgeRegressor  # Deterministic regression model, e.g., RidgeRegressor, Lasso, etc
    #mdl  # Deterministic regression model, e.g., RidgeRegressor, Lasso, etc
end


MMI.supports_weights(model::Type{<:SingleKernelRegressor}) = true

function MMI.fit(m::SingleKernelRegressor, verbosity::Int, X, y, w=nothing)

    #default behaviour is to normalize λ_kern to std dev of kernel matrix
    if m.λ_kern==999.0
        dKt = KernelFunctions.compose(PnormKernelStd(m.p), ScaleTransform(1)) 
        dKKt=kernelmatrix(dKt, Tables.matrix(X), obsdim=1)
        m.λ_kern=1.0/(Statistics.std(dKKt))
    end
    
    #employ kernel trick into vectored space
    K_fit = PnormKernelFunction(X, p=m.p, λ_kern=m.λ_kern)
    #weight/scale X based on passed wts otherwise skip
    if w!=nothing K_fit = table(matrix(K_fit).*w') end
    
    #create machine based on passed model and parameters
    mach = machine(m.mdl, K_fit, y)

    #fit the machine, set verbosity to 0 to reduce messages
    core_fitresult = MLJ.fit!(mach, verbosity=0)

    #return fitresult, Xold, cache, report
    cache  = fitted_params(mach) #nothing
    report = nothing #report(mach) #nothing
    return (core_fitresult, X, w), cache, report
end

function MMI.predict(m::SingleKernelRegressor, fitresult, Xnew)
    #pull out core fitresult and X's that generated the kernel
    core_fitresult, Xold, w = fitresult
    
    #employ kernel trick into RHKS space, combining Xold with Xnew
    #require "new" data to be first
    #note that converted from table/dataframe to matrix and back to table
    #K_pred = MLJ.table(kernelmatrix(K, Matrix(Xnew), Matrix(Xold), obsdim=1))
    K_pred = PnormKernelFunction(Xnew, Xold, p=m.p, λ_kern=m.λ_kern)
    
    #scale X if wts exists
    if w!=nothing K_pred = table(matrix(K_pred).*w') end
    
    return MLJ.predict(core_fitresult, K_pred)
end

###############################################################################
# Multi-kernel Regressor constructor based on MLJ.jl
#
# Once constructed, have standard 'fit' and 'predict' function calls
#
#https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/
#https://discourse.julialang.org/t/mlj-selecting-rows-and-columns-for-training-in-evaluate-for-kernel-regression/52832/8
#
#this is essentially a module if add in proper using/imports
###############################################################################

#const MMI = MLJBase #MLJModelInterface
const MMI = MLJModelInterface

#set up basic model structure, clean! and keyword construct using macro method
#basically a wrapper
@mlj_model mutable struct MultipleKernelRegressor <: MMI.Deterministic
    p::Array{Float64,2}      = [1.0 1.0 1.0]::all(_ .≥ zeros(Float64, size(_)))      # array of "p" for p-norm distance, each for measurement type/kernel
    λ_kern::Array{Float64,2} = [999.0 999.0 999.0]::all(_ .≥ zeros(Float64, size(_)))# array of kernel scale params for each measurement applied prior to kernelization
    f_counts::Array{Int64,2} = [112 16 8]::all(_ .≥ ones(Int64, size(_)))            # number of measurement-specific features, each measurement type used for kernel
    mdl::Deterministic=RidgeRegressor                         # Deterministic regression model, e.g., RidgeRegressor, Lasso, etc
end


function MMI.fit(m::MultipleKernelRegressor, verbosity, X, y, w=nothing)
    
    #cumulative sum of measurements, used to pull out each measurement type for kernel
    f_sum = cumsum(m.f_counts, dims=2)

    #default behaviour is to normalize λ_kern to std dev of kernel matrix
    #iterate through each measurement type
    for idx in 1:length(m.λ_kern)
        if m.λ_kern[idx] == 999.0
            dKt = KernelFunctions.compose(PnormKernelStd(m.p[idx]), ScaleTransform(1))
            if idx == 1
                dKKt=kernelmatrix(dKt, Tables.matrix(X[:,1:f_sum[idx]]), obsdim=1)
            else
                dKKt=kernelmatrix(dKt, Tables.matrix(X[:,f_sum[idx-1]:f_sum[idx]]), obsdim=1)
            end
            m.λ_kern[idx]=1.0/(Statistics.std(dKKt))
        end
    end
        
    #employ kernel trick into vectored space, cycle over the different feature kernels
    #note that converted from table/dataframe to matrix and back to table
    K_fit=[]#DataFrames.DataFrame()
    for idx in 1:length(m.f_counts)
        #create first feature kernel
        if idx == 1
            #DataFrames.hcat!(K_fit, DataFrames.DataFrame(PnormKernelFunction(X[:,1:f_sum[idx]], p=p[idx], λ_kern=λ_kern[idx])))
            K_fit=matrix(PnormKernelFunction(X[:,1:f_sum[idx]], p=m.p[idx], λ_kern=m.λ_kern[idx]))
        #concatenate each subsequent feature kernel
        else
            #DataFrames.hcat!(K_fit, DataFrames.DataFrame(PnormKernelFunction(X[:,f_sum[idx-1]:f_sum[idx]], p=p[idx], λ_kern=λ_kern[idx])))
            K_fit=[K_fit matrix(PnormKernelFunction(X[:,f_sum[idx-1]:f_sum[idx]], p=m.p[idx], λ_kern=m.λ_kern[idx]))]
            #K_fit=K_fit.+ matrix(PnormKernelFunction(X[:,f_sum[idx-1]:f_sum[idx]], p=m.p[idx], λ_kern=m.λ_kern[idx]))
        end
    end
    
    #convert to table as expected
    K_fit=MLJ.table(K_fit)
    #weight/scale X based on passed wts otherwise skip
    if w!=nothing K_fit = MLJ.table(matrix(K_fit).*(w')) end

    
    #create machine based on passed model and parameters
    mach = machine(m.mdl, K_fit, y)

    #fit the machine, set verbosity to 0 to reduce messages
    core_fitresult = MLJ.fit!(mach, verbosity=0)

    #return fitresult, Xold, cache, report
    cache = fitted_params(mach)
    report = nothing
    return (core_fitresult, X, w), cache, report

end

function MMI.predict(m::MultipleKernelRegressor, fitresult, Xnew)
    #pull out core fitresult and X's that generated the kernel
    core_fitresult, Xold, w = fitresult
    
    #cumulative sum of measurements, used to pull out each measurement type for kernel
    f_sum = cumsum(m.f_counts, dims=2)
            
    #employ kernel trick into vectored space, combining Xold with Xnew
    #require "new" data to be first, cycle over feature sets
    #note that converted from table/dataframe to matrix and back to table
    #K_pred = MLJ.table(kernelmatrix(K, Matrix(Xnew), Matrix(Xold), obsdim=1))
    K_pred=[]#DataFrames.DataFrame()
    for idx in 1:length(m.f_counts)
        #create first feature kernel
        if idx ==1
            #DataFrames.hcat!(K_pred, DataFrames.DataFrame(PnormKernelFunction(Xnew[:,1:f_sum[idx]], Xold[:,1:f_sum[idx]], p=p[idx], λ_kern=λ_kern[idx])))
            K_pred=matrix(PnormKernelFunction(Xnew[:,1:f_sum[idx]], Xold[:,1:f_sum[idx]], p=m.p[idx], λ_kern=m.λ_kern[idx]))
        #concatenate each subsequent feature kernel
        else
            #DataFrames.hcat!(K_pred, DataFrames.DataFrame(PnormKernelFunction(Xnew[:,f_sum[idx-1]:f_sum[idx]], Xold[:,f_sum[idx-1]:f_sum[idx]], p=p[idx], λ_kern=λ_kern[idx])))
            K_pred=[K_pred matrix(PnormKernelFunction(Xnew[:,f_sum[idx-1]:f_sum[idx]], Xold[:,f_sum[idx-1]:f_sum[idx]], p=m.p[idx], λ_kern=m.λ_kern[idx]))]
            #K_pred=K_pred.+ matrix(PnormKernelFunction(Xnew[:,f_sum[idx-1]:f_sum[idx]], Xold[:,f_sum[idx-1]:f_sum[idx]], p=p[idx], λ_kern=λ_kern[idx]))
        end
    end
    
    #convert to table as expected
    K_pred=MLJ.table(K_pred)
    #scale X if wts exists
    if w!=nothing K_pred = table(matrix(K_pred).*(w')) end

    
    return MLJ.predict(core_fitresult, K_pred)
end

